{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42 files belonging to 7 classes.\n",
      "Found 42 files belonging to 7 classes.\n",
      "['1Hundrednote', '2Hundrednote', '2Thousandnote', '5Hundrednote', 'Fiftynote', 'Tennote', 'Twentynote']\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 3s 0us/step\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CHERUKURI GUNNESH\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:5714: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 11s 4s/step - loss: 2.0745 - accuracy: 0.1190 - val_loss: 1.5953 - val_accuracy: 0.3810\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.7522 - accuracy: 0.2857 - val_loss: 1.3375 - val_accuracy: 0.4524\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 1.3506 - accuracy: 0.4524 - val_loss: 0.9964 - val_accuracy: 0.7143\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.9918 - accuracy: 0.7381 - val_loss: 0.8161 - val_accuracy: 0.8571\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7729 - accuracy: 0.9286 - val_loss: 0.6657 - val_accuracy: 0.9524\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.6516 - accuracy: 0.9524 - val_loss: 0.5734 - val_accuracy: 0.9286\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.5732 - accuracy: 0.8810 - val_loss: 0.4946 - val_accuracy: 0.9048\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.4912 - accuracy: 0.9048 - val_loss: 0.3474 - val_accuracy: 1.0000\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.3135 - accuracy: 1.0000 - val_loss: 0.2766 - val_accuracy: 1.0000\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.2761 - accuracy: 1.0000 - val_loss: 0.2252 - val_accuracy: 1.0000\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.2232 - accuracy: 1.0000 - val_loss: 0.1790 - val_accuracy: 1.0000\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.1650 - accuracy: 1.0000 - val_loss: 0.1444 - val_accuracy: 1.0000\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.1347 - accuracy: 1.0000 - val_loss: 0.1207 - val_accuracy: 1.0000\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.1144 - accuracy: 1.0000 - val_loss: 0.1035 - val_accuracy: 1.0000\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.1029 - accuracy: 1.0000 - val_loss: 0.0844 - val_accuracy: 1.0000\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0789 - accuracy: 1.0000 - val_loss: 0.0710 - val_accuracy: 1.0000\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0659 - accuracy: 1.0000 - val_loss: 0.0595 - val_accuracy: 1.0000\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0593 - accuracy: 1.0000 - val_loss: 0.0499 - val_accuracy: 1.0000\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0471 - accuracy: 1.0000 - val_loss: 0.0439 - val_accuracy: 1.0000\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 0.0392 - val_accuracy: 1.0000\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 0.0310 - val_accuracy: 1.0000\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0298 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 1.0000\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 1.0000\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 0.0209 - val_accuracy: 1.0000\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0204 - accuracy: 1.0000 - val_loss: 0.0197 - val_accuracy: 1.0000\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 1.0000\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.0172 - val_accuracy: 1.0000\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.0154 - val_accuracy: 1.0000\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0135 - val_accuracy: 1.0000\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
      "Epoch 56/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 57/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
      "Epoch 58/60\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
      "Epoch 59/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
      "Epoch 60/60\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "batch = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "train_path = r\"D:\\Project new  dataset\\Test\"\n",
    "test_path = r\"D:\\Project new  dataset\\Test\"\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_path,\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_path,\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Normalize data by using a rescaling layer\n",
    "normalization_layer = preprocessing.Rescaling(1.0 / 255)\n",
    "\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model\n",
    "base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create a custom classification head\n",
    "Classifier = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(units=128, activation='relu'),\n",
    "    layers.Dense(units=7, activation='softmax')\n",
    "])\n",
    "\n",
    "Classifier.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fit the classifier\n",
    "epochs = 60\n",
    "r = Classifier.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=epochs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Define your data loading and preprocessing here\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model\n",
    "base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create a custom classification head\n",
    "Classifier = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(units=64, activation='relu'),  # Reduced units in the dense layer\n",
    "    layers.Dense(units=7, activation='softmax')\n",
    "])\n",
    "\n",
    "Classifier.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Reduce the number of epochs\n",
    "epochs = 50\n",
    "\n",
    "# Use aggressive data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    preprocessing.RandomRotation(0.2),\n",
    "    preprocessing.RandomZoom(0.2),\n",
    "    preprocessing.RandomContrast(0.2),\n",
    "])\n",
    "\n",
    "# Fit the classifier\n",
    "r = Classifier.fit(\n",
    "    train_ds.map(lambda x, y: (data_augmentation(x), y)),  # Apply data augmentation\n",
    "    validation_data=test_ds,\n",
    "    epochs=epochs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "REAL CURRENCY\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# List of image paths for different currency notes\n",
    "image_paths = [\n",
    "    r\"R:\\21VV1F0025\\Data1\\Test\\Real Currency\\50-2 (7).jpg\",\n",
    "    # Add more image paths for other currency notes here\n",
    "]\n",
    "\n",
    "for img_path in image_paths:\n",
    "    # Load and preprocess the test image\n",
    "    img = image.load_img(img_path, target_size=(img_height, img_width))\n",
    "    test_image = image.img_to_array(img)\n",
    "    test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "    # Use your classifier model to make predictions\n",
    "    result = Classifier.predict(test_image)\n",
    "    predicted_class_index = np.argmax(result, axis=1)\n",
    "    predicted_class = class_names[predicted_class_index[0]]  # Use without .item()\n",
    "\n",
    "    # Extract the true class from the image path (assuming the path structure is consistent)\n",
    "    true_class = img_path.split('\\\\')[-2]\n",
    "\n",
    "    # Calculate and print accuracy\n",
    "    is_correct = (true_class == predicted_class)\n",
    "    accuracy = 1 if is_correct else 0  # 1 if correct, 0 if incorrect\n",
    "\n",
    "    if accuracy == 1:\n",
    "        print(\"REAL CURRENCY\")\n",
    "    else:\n",
    "        print(\"FAKE CURRENCY\")\n",
    "\n",
    "    print(\"------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
